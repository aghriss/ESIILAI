\documentclass[aspectratio=169]{beamer}
\usetheme{Copenhagen}
\usecolortheme{seahorse}
\setbeamertemplate{caption}[numbered]
\useoutertheme{infolines}
\setbeamertemplate{footline}{\hspace*{1cm}\scriptsize{
		\hspace*{50pt} \hfill\insertframenumber\hspace*{.5cm}}\\
	\vspace{10pt}}
%Information to be included in the title page:
\title{Machine Learning and Fair AI}
\author{Ayoub Ghriss\\ University of Colorado, Boulder}
\date{November 9th, 2023}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, latexsym}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{pgfgantt}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{xcolor}
\usetikzlibrary{decorations.pathreplacing}
\definecolor{fc}{HTML}{1E90FF}
\definecolor{conv}{HTML}{FFA500}
\definecolor{pool}{HTML}{B22222}
\tikzset{fc/.style={black,draw=fc,thick,rectangle,minimum height=0.75cm}}
\tikzset{conv/.style={black,draw=conv,thick,rectangle,minimum height=0.75cm}}
\tikzset{pool/.style={black,draw=pool,thick,rectangle,minimum height=0.75cm}}
\definecolor{barblue}{RGB}{153,204,254}
\definecolor{groupblue}{RGB}{51,102,254}
\definecolor{linkred}{RGB}{165,0,33}
% \renewcommand\sfdefault{phv}
% \renewcommand\mddefault{mc}
% \renewcommand\bfdefault{bc}
\usepackage{xspace}
\input{commands.tex}
\begin{document}

\frame{\titlepage}

% \section{Overview}

\begin{frame}
	\frametitle{Artificial Intelligence and Machine Learning}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				% \pause
				\item Artificial Intelligence (AI): software or machines that can
				      automatically analyze, reason, and learn through new complex
				      tasks.
				      % \pause
				\item How to define ``Intelligence"?: Human vs rational approaches
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				% \pause
				\item Machine Learning (ML) is a set of methods that build
				      intelligent software (AI).
				      % \pause
				\item ML methods: Supervised, Unsupervised, Reinforcement Learning
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Schedule}
	%
	% A fairly complicated example from section 2.9 of the package
	% documentation. This reproduces an example from Wikipedia:
	% http://en.wikipedia.org/wiki/Gantt_chart
	%
	\begin{ganttchart}[
		canvas/.append style={fill=none, draw=black!5, line width=.75pt},
		hgrid style/.style={draw=black!5, line width=.75pt},
		vgrid={*1{draw=black!5, line width=.75pt}},
		% today=7,
		% today rule/.style={
		% 		draw=black!64,
		% 		dash pattern=on 3.5pt off 4.5pt,
		% 		line width=1.5pt
		% 	},
		% today label font=\small\bfseries,
		title/.style={draw=none, fill=none}, title label
		font=\bfseries\footnotesize, title label node/.append style={below=7pt},
		include title in canvas=false, bar label
		font=\mdseries\small\color{black!70}, bar label node/.append
		style={left=2cm}, bar/.append style={draw=none, fill=black!63}, bar
		incomplete/.append style={fill=barblue}, bar progress label
		font=\mdseries\footnotesize\color{black!70}, group incomplete/.append
		style={fill=groupblue}, group left shift=0, group right shift=0, group
		height=.3, group peaks tip position=0, group label node/.append
		style={left=.6cm}, group progress label font=\bfseries\small,
		link/.style={-latex, line width=1.5pt, linkred}, link label
		font=\scriptsize\bfseries, link label node/.append style={below left=-2pt
		and 0pt} ]{1}{18} \gantttitle[ title label node/.append style={below
		left=7pt and -3pt}]{Mins}{1} \gantttitlelist{10,20,30,40,50,60,70,80,90}{2}
		\\ \ganttbar[ progress=50, progress label text={},
		name=C1]{\textbf{Supervised} Learning}{1}{6} \\ \ganttbar[ progress=67,
		progress label text={}, name=C2]{\textbf{Unsupervised} Learning}{7}{10} \\
		\ganttbar[ progress=100, progress label text={},
		name=C2]{\textbf{Reinforcement} Learning}{11}{13} \\ \ganttbar[
		progress=50, progress label text={}, name=C3]{\textbf{Fair} ML}{14}{18}
	\end{ganttchart}
\end{frame}

\section{Supervised Learning}
\begin{frame}
	\tableofcontents[currentsection]
\end{frame}

\begin{frame}
	\frametitle{The supervised paradigm}
	\begin{itemize}
		\item A dataset $\mathcal{D}_n = {(x_1,y_1), \ldots, (x_n, y_n)}$, $x_i$ is
		      the sample's features and $y_i$ its label.
		\item We assume that the labels $y_i$ are generated by some unknown
		      function (oracle) $f$: $y_i = f(x_i)$
		\item The \textbf{goal} is to learn an approximation $\hat{f}$ of $f$ based
		      on $\mathcal{D}_n$
		\item The approximation quality is evaluated based on a loss function $L$
		      where the total loss is
		      \[
			      \sum_i L(\hat{f}(x_i), y_i)
		      \]
	\end{itemize}
	The choice of a suitable algorithm depends on all the elements above (and more).
\end{frame}

\begin{frame}
	\frametitle{Supervised Examples: Computer Vision (CV)}
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{figures/vision.png}
		\caption{When the input samples are images}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Supervised Examples: Natural Language Processing (NLP)}
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{figures/speech.png}
		\caption{Sequence-to-Sequence matching}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Supervised Examples: Automatic Speech Recognition (ASR)}
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{figures/translate.png}
		\caption{Sequence-to-Sequence matching}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Supervised Learning: Bias-Variance}
	All supervised algorithms face a fundamental challenge:
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{figures/regression.png}
		\caption{Over/under-fitting in regression}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Supervised Learning: Bias-Variance}
	All supervised algorithms face a fundamental challenge:
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{figures/classification.png}
		\caption{Over/under-fitting in classification}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Supervised Learning: Decision Tree}

	\begin{figure}
		\includegraphics[width=0.6\textwidth]{figures/decision.png}
		\caption{Decision Trees (datacamp.com)}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Supervised Learning: Perceptron}
	\begin{figure}
		\includegraphics[width=0.6\textwidth]{figures/perceptron.png}
		\caption{Perceptron}\label{fig:}
	\end{figure}
\end{frame}
\section{Supervised Learning Practice}
\begin{frame}
	\frametitle{Supervised Learning: Image Classification}
	\begin{figure}
		\includegraphics[width=0.6\textwidth]{figures/catdog.png}
		\caption{Image Classification}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Supervised Learning: Convolution}
	\begin{figure}
		\includegraphics[width=0.7\textwidth]{figures/conv_pre.png}
		\caption{Image Classification}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Supervised Learning: Convolution Solution}
	\begin{figure}
		\includegraphics[width=0.7\textwidth]{figures/conv.png}
		\caption{Image Classification}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Supervised Learning: Stride}
	\begin{figure}
		\includegraphics[width=0.7\textwidth]{figures/small_conv.png}
		\caption{Stride}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Neural Networks: Output}
	\begin{itemize}
		\item The input of the neural network are the images (RGB)
		\item The output is a probability tuple $(p_{cat}, p_{dog})$
		\item Label $y=0$ if cat, $y=1$ if dog
		\item The loss function
		      \[
			      L(y, (p_{cat}, p_{dog})) = -(1-y) \log(p_{cat}) - y \log(p_{dog})
		      \]
	\end{itemize}
	\pause
	\begin{align*}
		L(0, (p_{cat}, p_{dog})) & = -\log(p_{cat}) \\
		L(1, (p_{cat}, p_{dog})) & = -\log(p_{dog})
	\end{align*}

	\pause
	$\rightarrow$ How do we transform the output of the neural network to probabilities?
\end{frame}

\begin{frame}
	\frametitle{Neural Networks: Activations}
	\begin{figure}
		\includegraphics[width=0.6\textwidth]{figures/activations.png}
		\caption{Stride}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Neural Networks: Activations}
	\begin{figure}
		\includegraphics[width=0.6\textwidth]{figures/activations1.png}
		\caption{Stride}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Neural Networks: Pooling}
	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/pooling.png}
		\caption{Pooling Operations}\label{fig:}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Neural Networks: Training}
	$rightarrow$ How do we find the optimal convolutions? (filters)
	\pause

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/sgd.png}
		\caption{Gradient Descent}\label{fig:}
	\end{figure}
\end{frame}

\section{Unsupervised Learning}
\begin{frame}
	\tableofcontents[currentsection]
\end{frame}

\begin{frame}
	\frametitle{The unsupervised paradigm}
	\begin{itemize}
		\item A dataset $\mathcal{D} = {(x_1), \ldots, (x_n)}$, $x_i$ is the
		      sample's features.
		\item There are no "labels".
		\item The goal is to learn a mapping $f: x\mapsto z$ where $z$ is in a
		      lower dimension space.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The nature of the representation $z$}
	\begin{itemize}
		\item The properties of the learned representation defines the algorithm
		\item If $z$ is in a discrete finite set: clustering or sparse coding
		\item Continuous: dimension reductions, generative models...
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Unsupervised Learning: Auto-encoders}
	We'll use the jupyter notebook for this.
\end{frame}
\section{Fairness}

\begin{frame}
	\frametitle{Fair ML}
	\begin{itemize}
		\item The question of fairness arises when the ML model is used for crucial
		      and impactful applications.
		\item For instance, is our pet classifier "fair"?: We had to build
		      auto-feeder for our pets
	\end{itemize}
\end{frame}

\end{document}
